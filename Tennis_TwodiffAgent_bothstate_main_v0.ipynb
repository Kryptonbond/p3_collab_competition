{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check_GPU = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#check_GPU\n",
    "#torch.cuda.get_device_name(-1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p3_collab_competition/Tennis_Linux/Tennis.x86_64\") #Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(state_size, action_size, num_agents=num_agents, random_seed=0)\n",
    "\n",
    "\n",
    "# def ddpg(n_episodes=10000, max_t=1000, print_every=100, train=True):\n",
    "#     scores_window = deque(maxlen=100)\n",
    "#     scores = []\n",
    "    \n",
    "#     for i_episode in range(1, n_episodes+1):\n",
    "#         env_info = env.reset(train_mode=True)[brain_name]   \n",
    "#         num_agents = len(env_info.agents)\n",
    "#         states = env_info.vector_observations\n",
    "#         scores_t = np.zeros(num_agents)\n",
    "#         agent.reset()\n",
    "        \n",
    "#         for t in range(max_t):            \n",
    "#             #actions = agent.act(states if train else np.zeros(states.size()))\n",
    "#             actions = agent.act(states, i_episode)\n",
    "#             env_info = env.step(actions)[brain_name]            \n",
    "#             next_states = env_info.vector_observations\n",
    "#             rewards = env_info.rewards\n",
    "#             dones = env_info.local_done\n",
    "            \n",
    "#             if train:\n",
    "#                 agent.step(states, actions, rewards, next_states, dones, t)\n",
    "#                 states = next_states\n",
    "#                 scores_t += np.array(rewards)\n",
    "#             if np.any(dones):\n",
    "#                 break\n",
    "        \n",
    "#         score = np.mean(scores_t)   \n",
    "#         scores_window.append(score)\n",
    "#         avg_score = np.mean(scores_window)\n",
    "#         scores.append(score)\n",
    "     \n",
    "\n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMean current: {:.2f}'.format(i_episode, avg_score, score), end=\"\")\n",
    "#         torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "#         torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "#         if i_episode % 10 == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "#         if avg_score >= 0.8:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "#             torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "#             torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "#             #break\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentLeft = Agent(state_size*2, action_size, num_agents=1, random_seed=0)\n",
    "agentRight = Agent(state_size*2, action_size, num_agents=1, random_seed=0)\n",
    "\n",
    "\n",
    "def ddpg(n_episodes=5000, max_t=2000, print_every=100, train=True):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores_t_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]   \n",
    "        num_agents = 2 #len(env_info.agents)\n",
    "        states = env_info.vector_observations\n",
    "        states = np.reshape(states, (1, 48)) #sam\n",
    "        \n",
    "        scores_t = np.zeros(num_agents)\n",
    "        agentLeft.reset()\n",
    "        agentRight.reset()\n",
    "        \n",
    "        for t in range(max_t):            \n",
    "            #actions = agent.act(states if train else np.zeros(states.size()))\n",
    "            actionsLeft = agentLeft.act(states, i_episode)#[0] #sam\n",
    "            #print(actionsLeft.shape , actionsLeft)\n",
    "            actionsRight = agentRight.act(states, i_episode)#[1] #sam\n",
    "            #print(actionsRight.shape , actionsRight)\n",
    "\n",
    "            #actions  = np.concatenate((actionsLeft, actionsRight) , axis=0) #np.array(actionsLeft, actionsLeft)\n",
    "            actions  = np.vstack((actionsLeft, actionsRight))\n",
    "            #print(actions.shape , actions)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]            \n",
    "            next_states = env_info.vector_observations\n",
    "            next_states = np.reshape(next_states, (1, 48)) #sam\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            #Sending (S, A, R, S) info to DDPG training agent for replay buffer and Neural network updates\n",
    "            if train:\n",
    "                agentLeft.step(states, actions[0], rewards[0], next_states, dones[0], t)\n",
    "                agentRight.step(states, actions[1], rewards[1], next_states, dones[1], t)\n",
    "                states = next_states\n",
    "                scores_t += np.array(rewards)\n",
    "#                 if rewards != [0.0, 0.0]:\n",
    "#                     print(\"Rewards\",rewards )\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        score = np.max(scores_t)  #score = np.mean(scores_t)   \n",
    "        scores_window.append(score)\n",
    "        scores_t_window.append(scores_t)\n",
    "        avg_score = np.mean(scores_window)\n",
    "        scores.append(score)\n",
    "        max_score = np.max(scores)\n",
    "     \n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMean current: {:.2f}\\tMax current: {:.2f}'.format(i_episode, avg_score, score, max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverageScoreS: {:.2f}'.format(i_episode, avg_score))\n",
    "            torch.save(agentLeft.actor_local.state_dict(), 'checkpoint_actorleft.pth')\n",
    "            torch.save(agentRight.actor_local.state_dict(), 'checkpoint_actorright.pth')\n",
    "\n",
    "            torch.save(agentLeft.critic_local.state_dict(), 'checkpoint_criticleft.pth')\n",
    "            torch.save(agentRight.critic_local.state_dict(), 'checkpoint_criticright.pth')\n",
    "            \n",
    "        if len(scores) >= 100 and i_episode % 100 == 0:\n",
    "            #summary += f', Score: {score:.2f}'\n",
    "            scores_filename = \"./data/Scores_2Agent_BothState\" +str(i_episode) + \".csv\"\n",
    "            np.savetxt(scores_filename, scores_window,  delimiter=\",\")          \n",
    "        if avg_score >= 0.8:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "            torch.save(agentLeft.actor_local.state_dict(), 'checkpoint_actorleft_best.pth')\n",
    "            torch.save(agentRight.actor_local.state_dict(), 'checkpoint_actorright_best.pth')\n",
    "\n",
    "            torch.save(agentLeft.critic_local.state_dict(), 'checkpoint_criticleft_best.pth')\n",
    "            torch.save(agentRight.critic_local.state_dict(), 'checkpoint_criticright_best.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 20\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 30\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 40\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 50\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 60\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 70\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 80\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 90\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 100\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 110\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 120\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 130\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 140\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 150\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 160\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 170\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 180\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 190\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 200\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 210\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 220\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 230\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 240\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 250\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 260\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 270\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 280\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 290\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 300\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 310\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 320\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 330\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 340\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 350\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 360\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 370\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 380\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 390\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 400\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 410\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 420\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 430\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 440\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 450\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 460\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 470\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 480\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 490\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 500\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 510\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 520\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 530\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 540\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 550\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 560\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 570\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.00\n",
      "Episode 580\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.09\n",
      "Episode 590\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.09\n",
      "Episode 600\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.09\n",
      "Episode 610\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.09\n",
      "Episode 620\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.09\n",
      "Episode 630\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 640\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 650\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 660\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 670\tAverageScoreS: 0.01\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 680\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 690\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 700\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 710\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 720\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 730\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 740\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 750\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 760\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 770\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 780\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 790\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 800\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 810\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 820\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 830\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 840\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 850\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 860\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 870\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 880\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 890\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 900\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 910\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 920\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 930\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 940\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 950\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 960\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 970\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 980\tAverageScoreS: 0.00\tMean current: 0.09\tMax current: 0.10\n",
      "Episode 990\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 1000\tAverageScoreS: 0.00\tMean current: 0.00\tMax current: 0.10\n",
      "Episode 1010\tAverageScoreS: 0.01\tMean current: 0.09\tMax current: 0.10\n",
      "Episode 1020\tAverageScoreS: 0.01\tMean current: 0.09\tMax current: 0.19\n",
      "Episode 1030\tAverageScoreS: 0.01\tMean current: 0.09\tMax current: 0.19\n",
      "Episode 1040\tAverageScoreS: 0.01\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1050\tAverageScoreS: 0.01\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1060\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1070\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1080\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1090\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1100\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1110\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1120\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1130\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1140\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1150\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1160\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1170\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1180\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1190\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1200\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1210\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1220\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1230\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1240\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1250\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1260\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1270\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1280\tAverageScoreS: 0.02\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1290\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1300\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1310\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1320\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1330\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1340\tAverageScoreS: 0.02\tMean current: 0.09\tMax current: 0.19\n",
      "Episode 1350\tAverageScoreS: 0.02\tMean current: 0.09\tMax current: 0.19\n",
      "Episode 1360\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1370\tAverageScoreS: 0.02\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1380\tAverageScoreS: 0.02\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1390\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1400\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1410\tAverageScoreS: 0.03\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1420\tAverageScoreS: 0.03\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1430\tAverageScoreS: 0.03\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1440\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1450\tAverageScoreS: 0.02\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1460\tAverageScoreS: 0.03\tMean current: 0.09\tMax current: 0.19\n",
      "Episode 1470\tAverageScoreS: 0.03\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1480\tAverageScoreS: 0.03\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1490\tAverageScoreS: 0.04\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1500\tAverageScoreS: 0.04\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1510\tAverageScoreS: 0.04\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1520\tAverageScoreS: 0.05\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1530\tAverageScoreS: 0.05\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1540\tAverageScoreS: 0.06\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1550\tAverageScoreS: 0.06\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1560\tAverageScoreS: 0.07\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1570\tAverageScoreS: 0.07\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1580\tAverageScoreS: 0.06\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1590\tAverageScoreS: 0.06\tMean current: 0.00\tMax current: 0.19\n",
      "Episode 1600\tAverageScoreS: 0.06\tMean current: 0.10\tMax current: 0.19\n",
      "Episode 1608\tAverage Score: 0.06\tMean current: 0.09\tMax current: 0.19"
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('Average_score_over100_episodes_p3_v1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)n_\n",
    "# n_episodes = 1000\n",
    "# for i_episode in range(1, n_episodes+1):\n",
    "#     actions = agent.act(states, i_episode)                        # select an action (for each agent)\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
