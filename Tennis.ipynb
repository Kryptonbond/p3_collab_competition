{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check_GPU = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#check_GPU\n",
    "#torch.cuda.get_device_name(-1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p3_collab_competition/Tennis_Linux/Tennis.x86_64\") #Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(state_size, action_size, num_agents=num_agents, random_seed=0)\n",
    "\n",
    "\n",
    "# def ddpg(n_episodes=10000, max_t=1000, print_every=100, train=True):\n",
    "#     scores_window = deque(maxlen=100)\n",
    "#     scores = []\n",
    "    \n",
    "#     for i_episode in range(1, n_episodes+1):\n",
    "#         env_info = env.reset(train_mode=True)[brain_name]   \n",
    "#         num_agents = len(env_info.agents)\n",
    "#         states = env_info.vector_observations\n",
    "#         scores_t = np.zeros(num_agents)\n",
    "#         agent.reset()\n",
    "        \n",
    "#         for t in range(max_t):            \n",
    "#             #actions = agent.act(states if train else np.zeros(states.size()))\n",
    "#             actions = agent.act(states, i_episode)\n",
    "#             env_info = env.step(actions)[brain_name]            \n",
    "#             next_states = env_info.vector_observations\n",
    "#             rewards = env_info.rewards\n",
    "#             dones = env_info.local_done\n",
    "            \n",
    "#             if train:\n",
    "#                 agent.step(states, actions, rewards, next_states, dones, t)\n",
    "#                 states = next_states\n",
    "#                 scores_t += np.array(rewards)\n",
    "#             if np.any(dones):\n",
    "#                 break\n",
    "        \n",
    "#         score = np.mean(scores_t)   \n",
    "#         scores_window.append(score)\n",
    "#         avg_score = np.mean(scores_window)\n",
    "#         scores.append(score)\n",
    "     \n",
    "\n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMean current: {:.2f}'.format(i_episode, avg_score, score), end=\"\")\n",
    "#         torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "#         torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "#         if i_episode % 10 == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "#         if avg_score >= 0.8:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "#             torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "#             torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "#             #break\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p3_collab_competition/model.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc1.weight) ### added a new type of weights Initalization as read.\n",
      "/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p3_collab_competition/model.py:69: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fcs1.weight)\n"
     ]
    }
   ],
   "source": [
    "agentLeft = Agent(state_size*2, action_size, num_agents=1, random_seed=0)\n",
    "agentRight = Agent(state_size*2, action_size, num_agents=1, random_seed=0)\n",
    "\n",
    "\n",
    "def ddpg(n_episodes=3000, max_t=1000, print_every=100, train=True):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores_t_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]   \n",
    "        num_agents = 2 #len(env_info.agents)\n",
    "        states = env_info.vector_observations\n",
    "        states = np.reshape(states, (1, 48)) #sam\n",
    "        \n",
    "        scores_t = np.zeros(num_agents)\n",
    "        agentLeft.reset()\n",
    "        agentRight.reset()\n",
    "        \n",
    "        for t in range(max_t):            \n",
    "            #actions = agent.act(states if train else np.zeros(states.size()))\n",
    "            actionsLeft = agentLeft.act(states, i_episode)#[0] #sam\n",
    "            #print(actionsLeft.shape , actionsLeft)\n",
    "            actionsRight = agentRight.act(states, i_episode)#[1] #sam\n",
    "            #print(actionsRight.shape , actionsRight)\n",
    "\n",
    "            #actions  = np.concatenate((actionsLeft, actionsRight) , axis=0) #np.array(actionsLeft, actionsLeft)\n",
    "            actions  = np.vstack((actionsLeft, actionsRight))\n",
    "            #print(actions.shape , actions)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]            \n",
    "            next_states = env_info.vector_observations\n",
    "            next_states = np.reshape(next_states, (1, 48)) #sam\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            #Sending (S, A, R, S) info to DDPG training agent for replay buffer and Neural network updates\n",
    "            if train:\n",
    "                agentLeft.step(states, actions[0], rewards[0], next_states, dones[0], t)\n",
    "                agentRight.step(states, actions[1], rewards[1], next_states, dones[1], t)\n",
    "                states = next_states\n",
    "                scores_t += np.array(rewards)\n",
    "#                 if rewards != [0.0, 0.0]:\n",
    "#                     print(\"Rewards\",rewards )\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        score = np.mean(scores_t)   \n",
    "        scores_window.append(score)\n",
    "        scores_t_window.append(scores_t)\n",
    "        avg_score = np.mean(scores_window)\n",
    "        scores.append(score)\n",
    "        max_score = np.max(scores)\n",
    "     \n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMean current: {:.2f}\\tMax current: {:.2f}'.format(i_episode, avg_score, score, max_score), end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverageScoreS: {:.2f}'.format(i_episode, avg_score))\n",
    "            torch.save(agentLeft.actor_local.state_dict(), 'checkpoint_actorleft.pth')\n",
    "            torch.save(agentRight.actor_local.state_dict(), 'checkpoint_actorright.pth')\n",
    "\n",
    "            torch.save(agentLeft.critic_local.state_dict(), 'checkpoint_criticleft.pth')\n",
    "            torch.save(agentRight.critic_local.state_dict(), 'checkpoint_criticright.pth')\n",
    "            \n",
    "        if len(scores) >= 100 and i_episode % 100 == 0:\n",
    "            #summary += f', Score: {score:.2f}'\n",
    "            scores_filename = \"./data/Scores_2Agent_BothState\" +str(i_episode) + \".csv\"\n",
    "            np.savetxt(scores_filename, scores_window,  delimiter=\",\")          \n",
    "        if avg_score >= 0.8:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_score))\n",
    "            torch.save(agentLeft.actor_local.state_dict(), 'checkpoint_actorleft_best.pth')\n",
    "            torch.save(agentRight.actor_local.state_dict(), 'checkpoint_actorright_best.pth')\n",
    "\n",
    "            torch.save(agentLeft.critic_local.state_dict(), 'checkpoint_criticleft_best.pth')\n",
    "            torch.save(agentRight.critic_local.state_dict(), 'checkpoint_criticright_best.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 20\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 30\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 40\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 50\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 60\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 70\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 80\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 90\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 100\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 110\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 120\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 130\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 140\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 150\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 160\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 170\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 180\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 190\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 200\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 210\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 220\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 230\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 240\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 250\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 260\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: -0.00\n",
      "Episode 270\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.050\n",
      "Episode 280\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 290\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 300\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 310\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 320\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 330\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 340\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 350\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 360\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 370\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 380\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 390\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 400\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 410\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 420\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 430\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 440\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 450\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 460\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 470\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 480\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 490\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 500\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 510\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 520\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 530\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 540\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 550\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 560\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 570\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 580\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 590\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 600\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 610\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 620\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 630\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 640\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 650\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 660\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 670\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 680\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 690\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 700\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 710\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 720\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 730\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 740\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 750\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 760\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 770\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 780\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 790\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 800\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 810\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 820\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 830\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 840\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 850\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 860\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 870\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 880\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 890\tAverageScoreS: -0.00\tMean current: 0.05\tMax current: 0.055\n",
      "Episode 900\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 910\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 920\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 930\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 940\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 950\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 960\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 970\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 980\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 990\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1000\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1010\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1020\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1030\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1040\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1050\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1060\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1070\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1080\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1090\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1100\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1110\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1120\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1130\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1140\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1160\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1170\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1180\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1190\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1200\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1210\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1220\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1230\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1240\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1250\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1260\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1270\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1280\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1290\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1300\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1310\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1320\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1330\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1340\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1350\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1360\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1370\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1380\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1390\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1400\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1410\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1420\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1430\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1440\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1450\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1460\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1470\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1480\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1490\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1500\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1510\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1520\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1530\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1540\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1550\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1560\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1570\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1580\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1590\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1600\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1610\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1620\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1630\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1640\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1650\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1660\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1670\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1680\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1690\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1700\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1710\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1720\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1730\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1740\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1750\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1760\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1770\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1780\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1790\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1800\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1810\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1820\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1830\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1840\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1850\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1860\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1870\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1880\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1890\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1900\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1910\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1920\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1930\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1940\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1950\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1960\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1970\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1980\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 1990\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2000\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2010\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2020\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2030\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2040\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2050\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2060\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2070\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2080\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2090\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2100\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2110\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2120\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2130\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2140\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2150\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2160\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2170\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2180\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2190\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2200\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2210\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2220\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2230\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2240\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2250\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2260\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2270\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2280\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2290\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2300\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2310\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2320\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2330\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2340\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2350\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2360\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2370\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2380\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2390\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2400\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2410\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2420\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2430\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2440\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2450\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2460\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2470\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2480\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2490\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2500\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2510\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2520\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2530\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2540\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2550\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2560\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2570\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2580\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2590\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2600\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2610\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2620\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2630\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2640\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2650\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2660\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2670\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2680\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2690\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2700\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2710\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2720\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2730\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2740\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2750\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2760\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2770\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2780\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2790\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2800\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2810\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2820\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2830\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2840\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2850\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2860\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2870\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2880\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2890\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2900\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2910\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2920\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2930\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2940\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2950\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2960\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2970\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2980\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 2990\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n",
      "Episode 3000\tAverageScoreS: -0.00\tMean current: -0.00\tMax current: 0.05\n"
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4fcbbd6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5b2f882b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('Average_score_over100_episodes_p3_v0.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f04e67d8318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_actor.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_critic.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m                  \u001b[0;31m# get the current state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)n_\n",
    "n_episodes = 1000\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    actions = agent.act(states, i_episode)                        # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
